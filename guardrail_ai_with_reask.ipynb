{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:28:25.537721500Z",
     "start_time": "2024-05-28T16:28:24.536486100Z"
    }
   },
   "outputs": [],
   "source": [
    "from guardrails import Guard\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the API key for Gemini"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def load_api_key():\n",
    "    with open('config.json') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        return config['apiKey']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:28:27.255762500Z",
     "start_time": "2024-05-28T16:28:27.250060900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# configuring the gemini model with the api key\n",
    "genai.configure(api_key=load_api_key())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:28:29.940293300Z",
     "start_time": "2024-05-28T16:28:29.936976300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuring the Gemini Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# configurations for the model\n",
    "gen_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:28:32.275316600Z",
     "start_time": "2024-05-28T16:28:32.268333800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# selecting the AI model for the response\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash-latest\",\n",
    "    generation_config=gen_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:28:34.545999500Z",
     "start_time": "2024-05-28T16:28:34.543044200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# for now there is no history for the chat\n",
    "chat_session = model.start_chat(\n",
    "    history=[\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:28:36.784837200Z",
     "start_time": "2024-05-28T16:28:36.781819Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining a custom LLM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def my_llm_api(\n",
    "    prompt: Optional[str] = None,\n",
    "    instruction: Optional[str] = None,\n",
    "    msg_history: Optional[list[dict]] = None,\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"Custom LLM API wrapper.\n",
    "\n",
    "    At least one of prompt, instruction or msg_history should be provided.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to be passed to the LLM API\n",
    "        instruction (str): The instruction to be passed to the LLM API\n",
    "        msg_history (list[dict]): The message history to be passed to the LLM API\n",
    "        **kwargs: Any additional arguments to be passed to the LLM API\n",
    "\n",
    "    Returns:\n",
    "        str: The output of the LLM API\n",
    "    \"\"\"\n",
    "\n",
    "    # Call your LLM API here\n",
    "    llm_output = chat_session.send_message(prompt).text\n",
    "\n",
    "    return llm_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:30:00.150139900Z",
     "start_time": "2024-05-28T16:30:00.137146400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the RAIL spec file to generate response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "guard = Guard.from_rail('./guardrail.xml')\n",
    "_, validated_output, *rest = guard(\n",
    "    my_llm_api,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:30:08.986617400Z",
     "start_time": "2024-05-28T16:30:04.268302600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fees': [{'index': 1, 'name': 'transaction fee', 'explanation': 'Fee for processing transactions', 'value': 1.5}, {'index': 2, 'name': 'monthly maintenance fee', 'explanation': 'Monthly fee for account maintenance', 'value': 5.0}, {'index': 3, 'name': 'overdraft fee', 'explanation': 'Fee for exceeding account balance', 'value': 35.0}], 'interest_rates': 'null'}\n"
     ]
    }
   ],
   "source": [
    "# print(guard)\n",
    "print(validated_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T16:30:12.011783300Z",
     "start_time": "2024-05-28T16:30:12.011079300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
