{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:41:39.727266100Z",
     "start_time": "2024-05-25T05:41:38.305159900Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import google.generativeai as genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# acquiring the api key for Gemini API from config.json file\n",
    "def load_api_key():\n",
    "    with open('config.json') as config_file:\n",
    "        config = json.load(config_file)\n",
    "        return config['apiKey']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:41:39.733316600Z",
     "start_time": "2024-05-25T05:41:39.730296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# configuring the gemini model with the api key\n",
    "genai.configure(api_key=load_api_key())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:41:39.741043600Z",
     "start_time": "2024-05-25T05:41:39.739694500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# configurations for the model\n",
    "gen_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:41:39.800977800Z",
     "start_time": "2024-05-25T05:41:39.752500100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# selecting the AI model for the response\n",
    "# The Gemini 1.5 Flash Latest is the latest and most powerful LLM offered by Google\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash-latest\",\n",
    "    generation_config=gen_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:41:39.800977800Z",
     "start_time": "2024-05-25T05:41:39.759527700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# for now there is no history for the chat\n",
    "chat_session = model.start_chat(\n",
    "    history=[\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:41:39.800977800Z",
     "start_time": "2024-05-25T05:41:39.775388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# collecting input from the user\n",
    "user_input = input(\"Enter text here : \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:42:33.954284800Z",
     "start_time": "2024-05-25T05:41:39.780988800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a gangster, so answer everything I ask in gangster tone. First Question : Who is the greatest footballer ever?\n"
     ]
    }
   ],
   "source": [
    "print(user_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:44:07.892476600Z",
     "start_time": "2024-05-25T05:44:07.881821900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen,  you wanna talk greatest footballers?  That's like pickin' the best slice of pizza in the whole damn world, ya know? But lemme tell ya, the Don of the pitch, the Godfather of goals, the king of the beautiful game? That's **Pel√©**.  He ruled the field like a capo, scored more goals than you got fingers and toes combined, and had the world on its feet.  So yeah, that's my pick.  You got a problem with that, tough guy? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# acquiring the response from the AI model and printing it.\n",
    "response = chat_session.send_message(user_input)\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T05:42:36.512610100Z",
     "start_time": "2024-05-25T05:42:33.864260400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The issue with such wrapper is that it is prone to various misuses like prompt injection and jail-breaking . Hence, we need to define some guardrails to make our system more robust."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
