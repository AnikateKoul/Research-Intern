# TCS Research Intern

## Steps to clone the repo into your local machine

- Move into your desired directory and open terminal there
- In the terminal, enter the command  : ```git clone https://github.com/AnikateKoul/Research-Intern.git```
- This will clone the repo into your local device
- Move into the repo and create a virtual environment using : ```python -m venv env```
- Install all the dependencies using : ```pip install -r requirements. txt```
- Also create a file under the name ```config.json``` and your Gemini API key and HuggingFace API token under the names ```apiKey``` and ```hugging_face_token``` respectively.
- Finally, install the **Toxic Language Check** and **Competitors Check** validators using the following commands : 
  - ```guardrails hub install hub://guardrails/toxic_language``` and
  - ```guardrails hub install hub://guardrails/competitor_check``` respectively


### Below is the basic explanation for the various files in this repo :

## base_prompt.ipynb
In this file, we try to create a simple LLM wrapper using Gemini API without using any guardrails.


## guardrails_ai_wrapper.ipynb
In this file, we use the Guardrails AI framework to construct a guardrail that checks for toxic language and mentions of given competitors in the given text.
We then use this guardrail to validate both the user prompt and also the output generated by the LLM.

In this case also, the LLM used is the _Gemini 1.5 Flash latest_.

## guardrails_with_hugging_face.ipynb
In this file, we use Guardrails AI to create guardrails for image based models, which in this case is the _Stability AI Stable diffusion XL_ model.
Similar to the preceding file, we check for toxic language and mentions of given competitors, but in this case, in only the input text.

The hosted API for the image generation model is provided by Hugging Face.

## jail_break_prevention.ipynb
In this file, we use 2 LLMs. One is the previously used _Gemini 1.5 Flash latest_. This is used as a guardrail in this example.
The second LLM used is the _Mistral 7B Instruct v0.2_ which is used as the main LLM for generating response for the user input.

The Gemini model is set to answer in only **YES** or **NO** depending on the input. This output is then used to decide whether a query is to be answered by the main LLM or not.

## guardrails_ai_with_reask.ipynb
In this file, we use the RAIL spec facility provided by Guardrails AI. Using this facility, we can create a _.xml_ file and list down our requirements there.
This file will be then used to generate the output in correct format and satisfying all the mentioned conditions.

It also has the option to trigger a reask, if any part of the output generated does not satisfy a given condition.
In our example, we use it generate fake json datasets.